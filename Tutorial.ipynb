{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c722113d-0405-4c47-bc69-9f154bc8020b",
   "metadata": {},
   "source": [
    "# How to use the DistilBERT.py (edit by xhu85@jhu.edu)\n",
    "\n",
    "It is introduction code of how to use the distilBERT.py for my first edition, you can change it after understanding how the code working\n",
    "\n",
    "## Import Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f32b2aa-e192-410a-bc26-2e1adf3b7f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoTokenizer, DefaultDataCollator, AutoModelForQuestionAnswering,\n",
    "                          TrainingArguments, Trainer, pipeline)\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeab50d8-effd-469c-9136-7489510cd7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_tokenizer_model_collator():\n",
    "    \"\"\"\n",
    "    Initialize new AutoTokenizer AutoModel Data collator\n",
    "    :return:\n",
    "    data_collator\n",
    "    tokenizerï¼š from AutoTokenizer\n",
    "    model: from AutoModelForQuestionAnswering\n",
    "    \"\"\"\n",
    "\n",
    "    data_collator = DefaultDataCollator()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "    return data_collator, tokenizer, model\n",
    "\n",
    "\n",
    "def training(output_dir: str, model: AutoModelForQuestionAnswering, train_dataset, test_dataset,\n",
    "             tokenizer: AutoTokenizer, data_collator: DefaultDataCollator, save_path):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy='epoch',\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_gpu_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        push_to_hub=False,  # no connection to Hugging HUb\n",
    "        report_to=['none']  # it require the set up of the wandb, will do it probably\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "\n",
    "\n",
    "def prepared_squad(tokenizer):\n",
    "    \"\"\"\n",
    "    It downloads and prepare the SQuAD(Stanford Question Answering Dataset) for training\n",
    "    :return:\n",
    "    tokenized_squad: tokenized SQuAD\n",
    "    \"\"\"\n",
    "    squad = load_dataset(\"squad\", split=\"train[:5000]\")\n",
    "    squad = squad.train_test_split(test_size=0.2)\n",
    "\n",
    "    def preprocess_function_squad(examples):\n",
    "        \"\"\"\n",
    "        It is a preprocessing example from the hugging hub\n",
    "        :param examples:\n",
    "        :param tokenizer:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        questions = [q.strip() for q in examples[\"question\"]]  # Strip the question\n",
    "        inputs = tokenizer(  # tokenize input\n",
    "            questions,\n",
    "            examples[\"context\"],\n",
    "            max_length=384,\n",
    "            truncation=\"only_second\",  # if len(questions+context) > max_input, only context will be truncated to fit\n",
    "            return_offsets_mapping=True,  # offset mapping in the tokenizers output, map token position to the character\n",
    "            # position in the original text\n",
    "            padding=\"max_length\",  # ensure all tokenized input are padded to the same length (max_length)\n",
    "        )\n",
    "\n",
    "        offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "        answers = examples[\"answers\"]\n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            answer = answers[i]\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "            sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "            # Find the start and end of the context\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "\n",
    "            # If the answer is not fully inside the context, label it (0, 0)\n",
    "            if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "                start_positions.append(0)\n",
    "                end_positions.append(0)\n",
    "            else:\n",
    "                # Otherwise it's the start and end token positions\n",
    "                idx = context_start\n",
    "                while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                    idx += 1\n",
    "                start_positions.append(idx - 1)\n",
    "\n",
    "                idx = context_end\n",
    "                while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                    idx -= 1\n",
    "                end_positions.append(idx + 1)\n",
    "\n",
    "        inputs[\"start_positions\"] = start_positions\n",
    "        inputs[\"end_positions\"] = end_positions\n",
    "        return inputs\n",
    "\n",
    "    tokenized_squad = squad.map(preprocess_function_squad, batched=True, remove_columns=squad[\"train\"].column_names)\n",
    "    return tokenized_squad, tokenizer\n",
    "\n",
    "\n",
    "def initialize_model_with_squad(save_path, model_name):\n",
    "    \"\"\"\n",
    "    Initialize the model\n",
    "    Train it with the SQuAD datasets and save it to setting directory\n",
    "    :param save_path: the directory we want our tokenizer and model saved to\n",
    "    :return: nothing return, model is saved under the certain directory\n",
    "\n",
    "    \"\"\"\n",
    "    data_collator, tokenizer, model = initialize_tokenizer_model_collator()\n",
    "    tokenized_squad, tokenizer = prepared_squad(tokenizer)\n",
    "    training(output_dir=model_name, model=model, train_dataset=tokenized_squad['train'],\n",
    "             test_dataset=tokenized_squad['test'], tokenizer=tokenizer, data_collator=data_collator,\n",
    "             save_path=save_path)\n",
    "\n",
    "\n",
    "def question_answer(model_path, question, context):\n",
    "    \"\"\"\n",
    "    Question Answer function\n",
    "    :param model_path: the path to the Directory of the tokenizer and the model\n",
    "    :param question: the string of the Question to the context\n",
    "    :param context: The text that you want to\n",
    "    :return: The Answer to the Question Related to the Context\n",
    "    \"\"\"\n",
    "    question_answerer = pipeline(\"question-answering\", model=model_path, tokenizer=model_path)\n",
    "    return question_answerer(question=question, context=context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae560a3-e101-470a-83a1-83c658036bc2",
   "metadata": {},
   "source": [
    "## Instruction part\n",
    "\n",
    "In the py, you can write those under\n",
    "\n",
    "<code>Python\n",
    "    if __name__ == \"__main__\":\n",
    "</code>\n",
    "\n",
    "Or when you import it for other code part\n",
    "\n",
    "<code>Python\n",
    "    import distilBERT\n",
    "    distilBERT.question_answer('./model/sample_model', 'How old is Tom?', 'Tom is 2 year old')\n",
    "</code>\n",
    "\n",
    "Do whatever you like!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1ce427-4dd3-422a-9cc4-735254f1116b",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "you need to initialize the model before there is no any other prior code\n",
    "\n",
    "The auto, tokenizer is imported from the <a herf='https://huggingface.co/distilbert/distilbert-base-uncased'>Huggingface</a>, there still some downstream task need to be done for pre-train the model\n",
    "\n",
    "<a herf='https://huggingface.co/datasets/rajpurkar/squad'>SQuAD</a> (Standford Question Answering Dataset) is used for the pre-train stage\n",
    "\n",
    "Default training parameter is under the\n",
    "\n",
    "<code>Python\n",
    "def training(output_dir: str, model: AutoModelForQuestionAnswering, train_dataset, test_dataset,\n",
    "             tokenizer: AutoTokenizer, data_collator: DefaultDataCollator, save_path):\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy='epoch',\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_gpu_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        push_to_hub=False,  # no connection to Hugging HUb\n",
    "        report_to=['none']  # it require the set up of the wandb, will do it probably\n",
    "    )\n",
    "    ........\n",
    "</code>\n",
    "                                                                                                                                                                              \n",
    "It using the              \n",
    "<ul>\n",
    "<li> Epoch, 3 epochs actually</li>\n",
    "<li>Learning Rate of 2e-5</li>\n",
    "<li>Push to hub is not allow here for privacy concern</li>\n",
    "<li>The wandb is banned for easy deploy</li>\n",
    "</ul>\n",
    "\n",
    "However if you want to push the model on the HuggingFace you need to login actually\n",
    "You can find instruction <a herf='https://huggingface.co/docs/huggingface_hub/quick-start'>Here</a>\n",
    "\n",
    "Wandb is very useful machine for observing the training loss and other metrics, however to use this you need the authentication and it has the risk of privacy, to deploy it you need this <a herf='https://docs.wandb.ai/guides/hosting/self-managed/basic-setup'>instruction</a>\n",
    "\n",
    "\n",
    "Let start the procedure to initialize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc6e24-8f69-41d2-8165-0e592bea3a5a",
   "metadata": {},
   "source": [
    "#### Indicate the path we store our model to \n",
    "\n",
    "A good practice is to always know where your model stored, since we need to import back our model for retraining and QA tas.\n",
    "\n",
    "Me, the developer tend to store it under the root directory, to model directory\n",
    "\n",
    "then Create a directory for this model specficlly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ce7e01d-2eb1-4a22-9819-51629c6cbb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './model/model_sample2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713303f3-1b84-4be9-b58a-68e5f7cf3f5f",
   "metadata": {},
   "source": [
    "#### Identify the model name\n",
    "\n",
    "not that necessary, just make sure not duplicate with other model under the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7f85992-625f-4116-9e12-1b5cfdee1865",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'qa_model2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b54825c1-0976-47a7-bc57-dec79ed9ca9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a421546dbd83492b8ec570f4d098cba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3f907757b942049bd07e64c4681286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10130\\anaconda3\\envs\\pytorch\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "C:\\Users\\10130\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\10130\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 07:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.838853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.237673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.166206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    }
   ],
   "source": [
    "initialize_model_with_squad(path, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c0b1f-afdd-45ad-8b72-2d6080d4280b",
   "metadata": {},
   "source": [
    "#### How to use the model?\n",
    "\n",
    "import the model from the path we store to , and then use the question, context and expect for an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f285a19-7756-472e-a041-7ffc0651cfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.14730927348136902, 'start': 10, 'end': 21, 'answer': '176 billion'}\n"
     ]
    }
   ],
   "source": [
    "question = 'How many programming languages does BLOOM support?'\n",
    "context = (\"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 \"\n",
    "               \"programming languages.\")\n",
    "print(question_answer(model_path=path, question=question,context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc4c0fa3-68f6-4cbb-a9c0-78119d1fe12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.14245392382144928, 'start': 122, 'end': 132, 'answer': '2.20 times'}\n"
     ]
    }
   ],
   "source": [
    "question = ('How much higher are the post-test odds of a high RDI compared to the pre-test odds following '\n",
    "                'a positive test?')\n",
    "context = ('Based on a moderate classification threshold from the boosting algorithm, the estimated post-test odds '\n",
    "               'of a high RDI were 2.20 times higher than the pre-test odds given a positive test, while the '\n",
    "               'corresponding post-test odds were decreased by 52% given a negative test (sensitivity and specificity '\n",
    "               'of 0.66 and 0.70, respectively).')\n",
    "print(question_answer(model_path=path, question=question, context=context))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb00b1-989b-4646-8005-4a6c50999af8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
